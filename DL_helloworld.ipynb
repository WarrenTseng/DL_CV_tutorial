{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will build a simple convolution network by *Tensorflow 2.0* to classify the handwritten digits dataset: MNIST\n",
    "\n",
    "Tensorflow: https://www.tensorflow.org/\n",
    "\n",
    "- **Requirements**\n",
    "    - python==3.6\n",
    "    - tensorflow==2.0\n",
    "    - numpy==1.17\n",
    "    - matplotlib==3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: prepare our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train[..., None], x_test[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can find that we have 60000 pictures of handwritten digits in the training set, and 10000 in the testing set.\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preview of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the usage of matplotlib.pyplot module is like MATLAB\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(x_train[i, ..., 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Digits: '+str(y_train[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply one-hot encoding to the labels <br>\n",
    "one-hot encoding: https://zhuanlan.zhihu.com/p/37471802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000,), array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before one-hot encoding\n",
    "y_train.shape, y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "y_train.shape, y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: build the convolutional neural network\n",
    "\n",
    "- The overall architecture: <br><br>\n",
    "<img src=\"./imgs/CNN_ALL_mnist.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input layer: <br><br>\n",
    "<img src=\"./imgs/CNN_xy_mnist.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conv layers: <br><br>\n",
    "<img src=\"./imgs/CNN_conv12conv2_mnist.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = tf.keras.layers.Conv2D(\n",
    "    filters=16, \n",
    "    kernel_size=(3, 3),\n",
    "    padding='same',\n",
    "    activation='relu'\n",
    ")\n",
    "norm1 = tf.keras.layers.BatchNormalization()\n",
    "pool1 = tf.keras.layers.MaxPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = tf.keras.layers.Conv2D(\n",
    "    filters=32, \n",
    "    kernel_size=(3, 3),\n",
    "    padding='same',\n",
    "    activation='relu'\n",
    ")\n",
    "norm2 = tf.keras.layers.BatchNormalization()\n",
    "pool2 = tf.keras.layers.MaxPool2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flatten layer: <br><br>\n",
    "<img src=\"./imgs/CNN_conv22flatten_mnist.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat3 = tf.keras.layers.Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dense (fully-connected) layer and output layer: <br><br>\n",
    "<img src=\"./imgs/CNN_hidden2output_mnist.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense4 = tf.keras.layers.Dense(\n",
    "    units=64,\n",
    "    activation='relu'\n",
    ")\n",
    "drop4 = tf.keras.layers.Dropout(rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the layers we built before together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    inputs,\n",
    "    conv1,\n",
    "    norm1,\n",
    "    pool1,\n",
    "    conv2,\n",
    "    norm2,\n",
    "    pool2,\n",
    "    flat3,\n",
    "    dense4,\n",
    "    drop4,\n",
    "    outputs\n",
    "]\n",
    "model = tf.keras.models.Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=1e-2,\n",
    "    momentum=.5\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                100416    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 106,058\n",
      "Trainable params: 105,962\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4757 - acc: 0.8515 - val_loss: 0.0998 - val_acc: 0.9695\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.2045 - acc: 0.9376 - val_loss: 0.0653 - val_acc: 0.9779\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 0.1558 - acc: 0.9535 - val_loss: 0.0568 - val_acc: 0.9806\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1301 - acc: 0.9611 - val_loss: 0.0466 - val_acc: 0.9845\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1127 - acc: 0.9663 - val_loss: 0.0467 - val_acc: 0.9856\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1003 - acc: 0.9707 - val_loss: 0.0435 - val_acc: 0.9861\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0892 - acc: 0.9740 - val_loss: 0.0375 - val_acc: 0.9878\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0829 - acc: 0.9754 - val_loss: 0.0351 - val_acc: 0.9889\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0738 - acc: 0.9783 - val_loss: 0.0385 - val_acc: 0.9878\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0716 - acc: 0.9781 - val_loss: 0.0341 - val_acc: 0.9896\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0674 - acc: 0.9795 - val_loss: 0.0370 - val_acc: 0.9880\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0644 - acc: 0.9808 - val_loss: 0.0354 - val_acc: 0.9886\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 0.0595 - acc: 0.9819 - val_loss: 0.0326 - val_acc: 0.9889\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0572 - acc: 0.9824 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0550 - acc: 0.9835 - val_loss: 0.0332 - val_acc: 0.9898\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0509 - acc: 0.9846 - val_loss: 0.0327 - val_acc: 0.9895\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0486 - acc: 0.9852 - val_loss: 0.0318 - val_acc: 0.9898\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0487 - acc: 0.9848 - val_loss: 0.0322 - val_acc: 0.9906\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0469 - acc: 0.9864 - val_loss: 0.0301 - val_acc: 0.9898\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0446 - acc: 0.9861 - val_loss: 0.0349 - val_acc: 0.9887\n"
     ]
    }
   ],
   "source": [
    "# we take the test set as validation set here\n",
    "h = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise: CIFAR 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
